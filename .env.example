# ===========================================================================
# Portfolio — Environment Variables
# Copy this file to .env and fill in the values.
#
#   cp .env.example .env
#
# Lines starting with # are comments and are ignored.
# ===========================================================================


# ===========================================================================
# APP
# ===========================================================================

# Runtime environment.  Controls cookie Secure flag, log verbosity, etc.
# Values: development | staging | production
ENVIRONMENT=development

# SQLAlchemy query logging — set to false in staging/production
DEBUG=true


# ===========================================================================
# DATABASE — PostgreSQL 16 + pgvector
# ===========================================================================

# Async connection string (asyncpg driver required)
# Docker Compose injects this automatically for the backend container.
# Override here only when running the backend locally without Docker.
DATABASE_URL=postgresql+asyncpg://portfolio:portfolio@localhost:5432/portfolio

POSTGRES_USER=portfolio
POSTGRES_PASSWORD=portfolio
POSTGRES_DB=portfolio

# ---------------------------------------------------------------------------
# Connection pool (sensible defaults for a single-server portfolio)
# ---------------------------------------------------------------------------

# Persistent connections kept open between requests
DB_POOL_SIZE=5

# Extra connections allowed during traffic spikes (pool_size + max_overflow = hard cap)
DB_MAX_OVERFLOW=10

# Seconds to wait for a free connection before raising TimeoutError
DB_POOL_TIMEOUT=30

# Recycle connections older than this many seconds (30 min) to avoid firewall
# idle-timeout cuts
DB_POOL_RECYCLE=1800


# ===========================================================================
# AUTH — JWT
# ===========================================================================

# HMAC signing secret for JWT tokens.
# Generate a strong value with:  python -c "import secrets; print(secrets.token_hex(64))"
# MUST be changed in staging / production.
JWT_SECRET=changeme-generate-a-strong-secret-before-deploying

# Signing algorithm — HS256 is fine for a single-server portfolio
JWT_ALGORITHM=HS256

# Token lifetime in minutes (default: 7 days)
JWT_EXPIRE_MINUTES=10080


# ===========================================================================
# AI PROVIDER — OpenAI OR vLLM
#
# The backend supports two AI providers.  Toggle with VLLM_ENABLED:
#
#   VLLM_ENABLED=false  →  uses OPENAI_BASE_URL + OPENAI_MODEL (real OpenAI)
#   VLLM_ENABLED=true   →  uses VLLM_BASE_URL + VLLM_MODEL (local vLLM)
# ===========================================================================

# ---------------------------------------------------------------------------
# Option A — Real OpenAI API (VLLM_ENABLED=false, default)
# ---------------------------------------------------------------------------

# Your OpenAI API key.  When using vLLM this can be any non-empty string.
OPENAI_API_KEY=sk-...

# OpenAI base URL.  Leave as-is for the real OpenAI API.
# Set to http://vllm:8000/v1 (or http://localhost:8001/v1 outside Docker)
# when VLLM_ENABLED=true.
OPENAI_BASE_URL=https://api.openai.com/v1

# Chat model used when VLLM_ENABLED=false
OPENAI_MODEL=gpt-4o


# ---------------------------------------------------------------------------
# Option B — Self-hosted vLLM  (VLLM_ENABLED=true)
#
# Target hardware: NVIDIA RTX 4080 — 16 GB VRAM (Ada Lovelace)
# Model: gpt-oss-20b (~20 B parameters)
#
# VRAM budget:
#   FP16 native   ~40 GB   ✗  does not fit
#   INT8          ~20 GB   ✗  too tight for KV cache
#   AWQ 4-bit     ~10 GB   ✓  leaves ~6 GB for KV cache  ← used
#
# AWQ (Activation-aware Weight Quantization) is vLLM's preferred 4-bit
# format: minimal accuracy loss, first-class CUDA kernel support.
# The model MUST be the AWQ-quantized variant from HuggingFace.
#
# Quick start:
#   1. Set VLLM_MODEL_HF_ID to the AWQ HuggingFace repo (see below)
#   2. Set VLLM_ENABLED=true
#   3. Set OPENAI_BASE_URL=http://vllm:8000/v1
#   4. Run:  task vllm:up
# ---------------------------------------------------------------------------

# Enable vLLM as the AI provider
VLLM_ENABLED=false

# HuggingFace repository that vLLM downloads and loads.
# MUST be an AWQ-quantized variant for 20 B on 16 GB VRAM.
# Update this once the exact repo path is confirmed, e.g.:
#   VLLM_MODEL_HF_ID=openai/gpt-oss-20b-AWQ
VLLM_MODEL_HF_ID=openai/gpt-oss-20b-AWQ

# Served model name — what the OpenAI-compatible API advertises.
# Must match --served-model-name in docker-compose.yml.
# The backend always calls model="gpt-oss-20b"; the HF repo path never
# leaks into application code.
VLLM_MODEL=gpt-oss-20b

# Base URL of the vLLM container's OpenAI-compatible endpoint.
# Use http://vllm:8000/v1 inside Docker Compose (service name resolution).
# Use http://localhost:8001/v1 when calling from outside Docker.
VLLM_BASE_URL=http://vllm:8000/v1

# ---------------------------------------------------------------------------
# vLLM tuning — RTX 4080 (16 GB VRAM) with AWQ 4-bit 20 B model
# ---------------------------------------------------------------------------

# Fraction of VRAM to allocate to vLLM (weights + KV cache).
# 0.90 → 14.4 GB used, ~1.6 GB reserved for CUDA context + overhead.
# Lower to 0.85 if you see OOM errors on first load.
VLLM_GPU_MEM_UTIL=0.90

# Maximum sequence length (prompt + generated tokens combined).
# With AWQ 4-bit + 16 GB, 8192 is safe.  Push to 16384 cautiously —
# every extra token increases KV cache memory usage.
VLLM_MAX_MODEL_LEN=8192

# HuggingFace access token.  Required only if VLLM_MODEL_HF_ID points to
# a gated (private) model.  Leave empty for public checkpoints.
HF_TOKEN=


# ===========================================================================
# CORS — allowed origins for the FastAPI CORS middleware
# ===========================================================================

# Comma-separated list is NOT supported by pydantic-settings for list fields.
# Edit app/core/config.py → CORS_ORIGINS if you need to add more origins.
# The defaults (localhost:3000 and localhost:5173) cover all dev scenarios.


# ===========================================================================
# FRONTEND — Vite environment variables
# All frontend env vars must be prefixed with VITE_ to be exposed to the
# browser bundle.
# ===========================================================================

# Backend API base URL.  Leave empty in development — the Vite proxy
# at /api forwards to http://localhost:8000 automatically (see vite.config.ts).
# Set to your deployed API URL in production, e.g. https://api.lhajoosten.dev
VITE_API_BASE_URL=
