# ===========================================================================
# Portfolio — Docker Compose
#
# Profiles:
#   (default)  db + backend + frontend
#   vllm       adds the vLLM OpenAI-compatible inference server
#
# Usage:
#   task dev           → docker compose up --build  (foreground)
#   task dev:d         → docker compose up --build -d  (background)
#   task dev:watch     → docker compose watch  (live file sync)
#   task vllm:up       → docker compose --profile vllm up vllm-chat infinity -d
#   task dev:full      → docker compose --profile vllm up --build
# ===========================================================================

services:
    # =========================================================================
    # DATABASE — PostgreSQL 16 + pgvector
    # =========================================================================
    db:
        image: pgvector/pgvector:pg16
        container_name: portfolio_db
        restart: unless-stopped
        environment:
            POSTGRES_USER: ${POSTGRES_USER:-portfolio}
            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-portfolio}
            POSTGRES_DB: ${POSTGRES_DB:-portfolio}
        ports:
            - "5432:5432"
        volumes:
            - postgres_data:/var/lib/postgresql/data
        healthcheck:
            test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-portfolio}"]
            interval: 5s
            timeout: 5s
            retries: 10

    # =========================================================================
    # BACKEND — FastAPI (Python 3.12 + uv)
    #
    # Single Dockerfile — dev CMD runs migrations + uvicorn --reload.
    # Source is synced into the container via `develop: watch:` so that
    # every save is reflected instantly without a full rebuild.
    # The .venv named volume prevents the bind-mount from wiping installed deps.
    # =========================================================================
    backend:
        build:
            context: ./backend
            dockerfile: Dockerfile
        container_name: portfolio_backend
        restart: unless-stopped
        env_file:
            - ./backend/.env
        environment:
            DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER:-portfolio}:${POSTGRES_PASSWORD:-portfolio}@db:5432/${POSTGRES_DB:-portfolio}
        ports:
            - "8000:8000"
        volumes:
            # Named volume preserves the .venv so dep installs survive source syncs
            - backend_venv:/app/.venv
        depends_on:
            db:
                condition: service_healthy
        develop:
            watch:
                # Sync application source on every save (fast — no rebuild)
                - action: sync
                  path: ./backend/app
                  target: /app/app
                  ignore:
                      - __pycache__/
                      - "*.pyc"
                      - "*.pyo"
                # Full rebuild when dependencies or config change
                - action: rebuild
                  path: ./backend/pyproject.toml
                - action: rebuild
                  path: ./backend/uv.lock

    # =========================================================================
    # FRONTEND — React 19 + Vite (Node 22 + pnpm)
    #
    # Single Dockerfile — dev CMD starts pnpm dev --host.
    # The node_modules named volume keeps the installed packages intact
    # while individual source files are synced via `develop: watch:`.
    # =========================================================================
    frontend:
        build:
            context: ./frontend
            dockerfile: Dockerfile
        container_name: portfolio_frontend
        restart: unless-stopped
        env_file:
            - ./frontend/.env
        ports:
            - "3000:3000"
        depends_on:
            - backend
        develop:
            watch:
                # Sync source files on every save (instant HMR via Vite)
                - action: sync
                  path: ./frontend/src
                  target: /app/src
                - action: sync
                  path: ./frontend/index.html
                  target: /app/index.html
                - action: sync
                  path: ./frontend/public
                  target: /app/public
                # Rebuild when package config changes
                - action: rebuild
                  path: ./frontend/package.json
                - action: rebuild
                  path: ./frontend/pnpm-lock.yaml
                - action: sync+restart
                  path: ./frontend/vite.config.ts
                  target: /app/vite.config.ts
                - action: sync+restart
                  path: ./frontend/tailwind.config.ts
                  target: /app/tailwind.config.ts

    # =========================================================================
    # vllm-chat — Writing assistant  (profile: vllm)
    #
    # Serves Qwen/Qwen2.5-7B-Instruct-AWQ via the OpenAI-compatible API.
    # Used by WritingService for SSE-streamed text generation in the CMS.
    #
    # Target hardware: NVIDIA RTX 4080  (16 GB VRAM / Ada Lovelace)
    #
    # VRAM budget:
    #   Qwen2.5-7B-Instruct-AWQ  →  ~4 GB weights  +  ~10 GB KV cache headroom
    #
    # Why 7B over 14B?
    #   The portfolio writing assistant does not need a 14B model.
    #   7B AWQ is fast, uses half the VRAM, and still produces excellent writing.
    #   The freed VRAM is available for the OS + CUDA context.
    #
    # Env vars (root .env):
    #   VLLM_CHAT_MODEL_HF_ID=Qwen/Qwen2.5-7B-Instruct-AWQ  ← HF repo to download
    #   VLLM_CHAT_MODEL=qwen2.5-7b                           ← served name
    #   HF_TOKEN=hf_...                                      ← only if model is gated
    #
    # Activate with:  task vllm:up   or   task dev:full
    # =========================================================================
    vllm-chat:
        image: vllm/vllm-openai:latest
        container_name: portfolio_vllm_chat
        profiles: [vllm]
        restart: unless-stopped
        environment:
            HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN:-}
        command:
            # ── Model ──────────────────────────────────────────────────────
            - "--model"
            - "${VLLM_CHAT_MODEL_HF_ID:-Qwen/Qwen2.5-7B-Instruct-AWQ}"

            # Served name: what the OpenAI-compatible API advertises.
            # The backend always calls model="qwen2.5-7b" — the HF path
            # never leaks into application code.
            - "--served-model-name"
            - "${VLLM_CHAT_MODEL:-qwen2.5-7b}"

            # ── Quantization ───────────────────────────────────────────────
            # AWQ 4-bit: ~4 GB weights for 7B, leaves ~12 GB for KV cache
            - "--quantization"
            - "awq"

            # FP16 activations + AWQ 4-bit weights — correct for Ada (RTX 40xx)
            - "--dtype"
            - "float16"

            # ── VRAM / KV cache ────────────────────────────────────────────
            # 85 % of 16 GB = ~13.6 GB.  7B AWQ only needs ~4 GB for weights;
            # the rest becomes KV cache.  Leaves ~2.4 GB for CUDA + OS overhead.
            - "--gpu-memory-utilization"
            - "${VLLM_GPU_MEM_UTIL:-0.85}"

            # 16 k context window — safe with 7B AWQ + 16 GB VRAM.
            # The writing assistant rarely needs more than 4–8 k tokens.
            - "--max-model-len"
            - "${VLLM_MAX_MODEL_LEN:-16384}"

            # ── Runtime ────────────────────────────────────────────────────
            - "--host"
            - "0.0.0.0"
            - "--port"
            - "8000"

            # Required by some community AWQ checkpoints that extend the
            # base tokenizer / modelling code
            - "--trust-remote-code"

            # Chunked prefill improves throughput for long prompts
            - "--enable-chunked-prefill"
        ports:
            # Host 8001 → container 8000 (avoids clash with FastAPI on 8000)
            - "8001:8000"
        volumes:
            # Cache downloaded weights across restarts.
            # First pull of Qwen2.5-7B-Instruct-AWQ is ~4 GB.
            - vllm_chat_cache:/root/.cache/huggingface
        # Shared memory required by vLLM tensor-parallel workers
        ipc: host
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: 1
                          capabilities: [gpu]
        healthcheck:
            test:
                ["CMD-SHELL", "curl -sf http://localhost:8000/health || exit 1"]
            interval: 30s
            timeout: 10s
            retries: 5
            # Allow up to 3 min for the model to load into VRAM on first start
            start_period: 180s

    # =========================================================================
    # infinity — Embedding server  (profile: vllm)
    #
    # Serves BAAI/bge-base-en-v1.5 via the OpenAI-compatible /v1/embeddings
    # endpoint.  Used by RagService to generate and query pgvector embeddings.
    #
    # Why infinity-emb instead of a second vLLM instance?
    #   vLLM is designed for generative models.  infinity-emb is purpose-built
    #   for embedding models: smaller image, faster startup, lower memory, and
    #   direct SentenceTransformers compatibility.
    #
    # Why bge-base-en-v1.5?
    #   768-dim vectors — smaller indices than 1024-dim alternatives, excellent
    #   English semantic search quality, ~440 MB model size, runs fast on CPU.
    #   The portfolio CMS embeds content on write (low frequency), so CPU is
    #   entirely sufficient and leaves all VRAM for vllm-chat.
    #
    # Model name in API calls: "BAAI/bge-base-en-v1.5"
    #   infinity-emb uses the HF repo path directly as the model identifier.
    #
    # Env vars (root .env):
    #   INFINITY_EMBED_MODEL=BAAI/bge-base-en-v1.5  (default shown)
    #
    # Activate with:  task vllm:up   or   task dev:full
    # =========================================================================
    infinity:
        image: michaelf34/infinity:latest
        container_name: portfolio_infinity
        profiles: [vllm]
        restart: unless-stopped
        command:
            - "v2"
            # HF model to download and serve.  The model name in API requests
            # must match this value exactly.
            - "--model-id"
            - "${INFINITY_EMBED_MODEL:-BAAI/bge-base-en-v1.5}"

            # Run on CPU — the embedding model is tiny (~440 MB) and this
            # keeps all GPU VRAM free for vllm-chat.
            - "--device"
            - "cpu"

            # Use the torch backend (default, most compatible)
            - "--engine"
            - "torch"

            - "--port"
            - "7997"
        ports:
            # Host 8002 → container 7997
            - "8002:7997"
        volumes:
            # Cache downloaded model weights across restarts (~440 MB download)
            - infinity_embed_cache:/app/.cache
        environment:
            HF_HOME: /app/.cache
        healthcheck:
            test:
                ["CMD-SHELL", "curl -sf http://localhost:7997/health || exit 1"]
            interval: 20s
            timeout: 10s
            retries: 5
            start_period: 60s

# =============================================================================
# Named volumes
# =============================================================================
volumes:
    # Postgres data directory — survives container restarts
    postgres_data:

    # Python virtual environment — survives source bind-mounts on the backend
    backend_venv:

    # Downloaded weights for the vLLM chat model (Qwen2.5-7B-Instruct-AWQ, ~4 GB)
    vllm_chat_cache:

    # Downloaded weights for the infinity embedding model (bge-base-en-v1.5, ~440 MB)
    infinity_embed_cache:
